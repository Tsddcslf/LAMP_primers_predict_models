{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ec640-8145-4d94-9180-22474ce008f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# 定义 API URL\n",
    "api_url = \"https://lampprimer.mathematik.uni-marburg.de/api.php?table=lamp&action=list\"\n",
    "\n",
    "# 下载数据\n",
    "response = requests.get(api_url,verify=False)\n",
    "\n",
    "# 检查响应状态码\n",
    "if response.status_code == 200:\n",
    "    print(\"数据成功下载！\")\n",
    "    \n",
    "    # 解析为 JSON 数据\n",
    "    data = response.json()  # 解析 API 响应为字典\n",
    "    print(f\"数据样本：\\n{json.dumps(data, indent=4)}\")  # 格式化打印数据\n",
    "    \n",
    "    # 保存到本地 JSON 文件\n",
    "    with open(\"1_primer_data.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "    print(\"数据已保存到 '1_primer_data.json'\")\n",
    "else:\n",
    "    print(f\"请求失败，状态码：{response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71138049-f062-4ba8-9d2d-46808c39c0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件编码为：utf-8\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "# 检测文件编码\n",
    "with open(\"1_primer_data.json\", \"rb\") as file:\n",
    "    raw_data = file.read()\n",
    "    result = chardet.detect(raw_data)\n",
    "    encoding = result['encoding']\n",
    "\n",
    "print(f\"文件编码为：{encoding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e1cebb-aba2-4418-971c-c0790155aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#在json文件中提取数据，并将数据存储到.csv文件中\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# 读取 JSON 文件\n",
    "data_file = \"1_primer_data.json\"\n",
    "csv_file = \"2_primer_data.csv\"\n",
    "\n",
    "# 定义目标字段\n",
    "fields = [\n",
    "    \"genbank\", \"F3_sequence\", \"F3_position\", \"B3_sequence\", \"B3_position\", \n",
    "    \"FIP_sequence\", \"FIP_position\", \"BIP_sequence\", \"BIP_position\", \n",
    "    \"LF_sequence\", \"LF_position\", \"LB_sequence\", \"LB_position\"\n",
    "]\n",
    "\n",
    "# 初始化 CSV 文件\n",
    "with open(data_file, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "with open(csv_file, \"w\", encoding=\"utf-8\", newline=\"\") as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # 遍历每个 data 键\n",
    "    for entry in data[\"data\"]:\n",
    "        row = {field: \"\" for field in fields}  # 初始化为空值\n",
    "\n",
    "        # 提取 genbank 值\n",
    "        primer_data = entry.get(\"primer\", [])\n",
    "        if isinstance(primer_data, list):\n",
    "            for primer_value in primer_data:\n",
    "                genbank = primer_value.get(\"genbank\")\n",
    "                if genbank and genbank != \"NA\":  # 找到非空且非 \"NA\" 的 genbank\n",
    "                    row[\"genbank\"] = genbank\n",
    "                    break\n",
    "\n",
    "            # 遍历 primer 数据，填充相应字段\n",
    "            for primer_value in primer_data:\n",
    "                name = primer_value.get(\"name\")\n",
    "                sequence = primer_value.get(\"sequence\")\n",
    "                position = primer_value.get(\"position\")\n",
    "\n",
    "                if name in [\"F3\", \"B3\", \"FIP\", \"BIP\", \"LF\", \"LB\"]:\n",
    "                    row[f\"{name}_sequence\"] = sequence if sequence else \"\"\n",
    "                    row[f\"{name}_position\"] = position if position else \"\"\n",
    "\n",
    "        # 写入行数据\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"数据已成功处理并保存到 {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71cb790-8af2-46fb-b923-23fa7db19fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#但是从json文件中提取的数据中有一些数据的FIP和BIP的position为空值，导致不能将其分为F2和F1或B2和B1。\n",
    "#也有些数据的genbank的值为空值，导致不能从数据库中下载到原始序列。\n",
    "#所以将这两种情况的数据视为无效数据，去除。\n",
    "\n",
    "data_to_keep = []\n",
    "csv_file = \"2_primer_data.csv\"\n",
    "processed_csv_file = \"3_primer_data.csv\"\n",
    "with open(csv_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    for row in reader:\n",
    "        # 去除 genbank 为空的行\n",
    "        if not row[\"genbank\"]:\n",
    "            continue\n",
    "\n",
    "        # 检查 FIP_sequence 和 BIP_sequence 的 position 是否为空\n",
    "        if (row[\"FIP_sequence\"] and not row[\"FIP_position\"]) or (row[\"BIP_sequence\"] and not row[\"BIP_position\"]):\n",
    "            continue\n",
    "\n",
    "        # 保留该行\n",
    "        data_to_keep.append(row)\n",
    "\n",
    "# 保存到新的 CSV 文件\n",
    "with open(processed_csv_file, \"w\", encoding=\"utf-8\", newline=\"\") as outfile:\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data_to_keep)\n",
    "\n",
    "print(f\"处理后的数据已成功保存到 {processed_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a3b4aa-33c9-4d18-a721-10d8cf2f259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据genbank号，从数据库中将原始基因序列下载下来。并删除原始序列为空的数据。\n",
    "\n",
    "import json\n",
    "import csv\n",
    "from Bio import Entrez, SeqIO\n",
    "from tqdm import tqdm  # 用于显示进度条\n",
    "\n",
    "# 设置邮箱地址 (NCBI 要求)\n",
    "Entrez.email = \"your_email@example.com\"\n",
    "\n",
    "# 从 NCBI 下载序列的函数\n",
    "def fetch_sequence_from_ncbi(genbank_id):\n",
    "    try:\n",
    "        with Entrez.efetch(db=\"nucleotide\", id=genbank_id, rettype=\"fasta\", retmode=\"text\") as handle:\n",
    "            record = SeqIO.read(handle, \"fasta\")\n",
    "            return str(record.seq)\n",
    "    except Exception as e:\n",
    "        print(f\"无法下载序列 {genbank_id}: {e}\")\n",
    "        return \"\"  # 如果下载失败返回空值\n",
    "\n",
    "# 输入和输出文件\n",
    "data_file = \"3_primer_data.csv\"\n",
    "output_file = \"4_primer_data.csv\"\n",
    "\n",
    "# 读取原始 CSV 文件并处理\n",
    "data_to_save = []\n",
    "with open(data_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fields = reader.fieldnames + [\"ori_sequence\"]  # 在原有字段后添加 \"ori_sequence\"\n",
    "\n",
    "    rows = list(reader)  # 将数据加载到列表中以计算总行数\n",
    "    for row in tqdm(rows, desc=\"Processing rows\", unit=\"row\"):\n",
    "        genbank_id = row.get(\"genbank\")\n",
    "        try:\n",
    "            if genbank_id and genbank_id != \"NA\":\n",
    "                row[\"ori_sequence\"] = fetch_sequence_from_ncbi(genbank_id)\n",
    "            else:\n",
    "                row[\"ori_sequence\"] = \"\"  # 如果 genbank 为空或无效\n",
    "        except Exception as e:\n",
    "            print(f\"跳过因错误无法处理的行: {e}\")\n",
    "            row[\"ori_sequence\"] = \"\"  # 防止程序中断，继续处理\n",
    "        data_to_save.append(row)\n",
    "\n",
    "# 保存到新的 CSV 文件\n",
    "with open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as outfile:\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data_to_save)\n",
    "\n",
    "print(f\"数据已成功处理并保存到 {output_file}\")\n",
    "\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# 设置较大的字段大小限制\n",
    "csv.field_size_limit(10**7)  # 设置为 10MB，可根据需要调整\n",
    "\n",
    "# 输入文件\n",
    "file_path = \"4_primer_data.csv\"\n",
    "\n",
    "# 清理数据并调整列顺序\n",
    "cleaned_data = []\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fields = reader.fieldnames\n",
    "\n",
    "    # 调整列顺序\n",
    "    reordered_fields = fields[:1] + [\"ori_sequence\"] + fields[1:-1]\n",
    "\n",
    "    for row in reader:\n",
    "        if row.get(\"ori_sequence\"):  # 只保留 ori_sequence 非空的行\n",
    "            cleaned_data.append(row)\n",
    "\n",
    "# 覆写原文件\n",
    "with open(file_path, \"w\", encoding=\"utf-8\", newline=\"\") as outfile:\n",
    "    writer = csv.DictWriter(outfile, fieldnames=reordered_fields)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(cleaned_data)\n",
    "\n",
    "print(f\"数据已成功清理并更新到 {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdfec98-d410-45dd-8282-9777612a7d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将F1和F2、B1和B2从FIP、BIP中提取出来，根据两者的position。\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# 增大 CSV 字段大小限制\n",
    "csv.field_size_limit(10**7)\n",
    "\n",
    "# 输入和输出文件\n",
    "input_file = \"4_primer_data.csv\"\n",
    "output_file = \"5_primer_data.csv\"\n",
    "\n",
    "# 解析位置格式函数\n",
    "def parse_positions(position_string):\n",
    "    if position_string == \"NA\":\n",
    "        return None, None\n",
    "    # 匹配两种格式的正则表达式\n",
    "    match = re.match(r\"(\\d+)-(\\d+)\\+(?:.*\\+)?(\\d+)-(\\d+)\", position_string)\n",
    "    if match:\n",
    "        start1, end1, start2, end2 = map(int, match.groups())\n",
    "        length1 = abs(end1 - start1) + 1\n",
    "        length2 = abs(end2 - start2) + 1\n",
    "        return length1, length2\n",
    "    return None, None\n",
    "\n",
    "# 提取序列函数\n",
    "def extract_subsequences(sequence, lengths):\n",
    "    if not sequence or not lengths or lengths[0] is None or lengths[1] is None:\n",
    "        return \"\", \"\"\n",
    "    length1, length2 = lengths\n",
    "    try:\n",
    "        seq1 = sequence[:length1]  # 提取前面部分\n",
    "        seq2 = sequence[-length2:]  # 提取后面部分\n",
    "        return seq1, seq2\n",
    "    except IndexError:\n",
    "        return \"\", \"\"  # 索引越界时返回空值\n",
    "\n",
    "# 处理文件\n",
    "data_to_save = []\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fields = reader.fieldnames\n",
    "\n",
    "    # 添加新列到字段列表\n",
    "    new_fields = fields[:]\n",
    "    new_fields.insert(new_fields.index(\"FIP_position\") + 1, \"F1_sequence\")\n",
    "    new_fields.insert(new_fields.index(\"F1_sequence\") + 1, \"F2_sequence\")\n",
    "    new_fields.insert(new_fields.index(\"BIP_position\") + 1, \"B1_sequence\")\n",
    "    new_fields.insert(new_fields.index(\"B1_sequence\") + 1, \"B2_sequence\")\n",
    "\n",
    "    for row in reader:\n",
    "        # 处理 FIP_sequence 和 FIP_position\n",
    "        fip_seq = row.get(\"FIP_sequence\", \"\")\n",
    "        fip_pos = row.get(\"FIP_position\", \"\")\n",
    "        if fip_pos == \"NA\":\n",
    "            row[\"F1_sequence\"] = \"\"\n",
    "            row[\"F2_sequence\"] = \"\"\n",
    "        else:\n",
    "            fip_lengths = parse_positions(fip_pos)\n",
    "            f1_seq, f2_seq = extract_subsequences(fip_seq, fip_lengths)\n",
    "            row[\"F1_sequence\"] = f1_seq\n",
    "            row[\"F2_sequence\"] = f2_seq\n",
    "\n",
    "        # 处理 BIP_sequence 和 BIP_position\n",
    "        bip_seq = row.get(\"BIP_sequence\", \"\")\n",
    "        bip_pos = row.get(\"BIP_position\", \"\")\n",
    "        if bip_pos == \"NA\":\n",
    "            row[\"B1_sequence\"] = \"\"\n",
    "            row[\"B2_sequence\"] = \"\"\n",
    "        else:\n",
    "            bip_lengths = parse_positions(bip_pos)\n",
    "            b1_seq, b2_seq = extract_subsequences(bip_seq, bip_lengths)\n",
    "            row[\"B1_sequence\"] = b1_seq\n",
    "            row[\"B2_sequence\"] = b2_seq\n",
    "\n",
    "        data_to_save.append(row)\n",
    "\n",
    "# 保存处理后的数据\n",
    "with open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as outfile:\n",
    "    writer = csv.DictWriter(outfile, fieldnames=new_fields)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data_to_save)\n",
    "\n",
    "print(f\"数据已成功处理并保存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c6a4a2-5a15-4392-b737-788e7c7265af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将数据的列进行筛选一下，只保留原始序列和八条引物序列。\n",
    "#数据中，有些引物数据为空值。LF和LB有较多空值，但空值还是占据少部分。\n",
    "\n",
    "import csv\n",
    "\n",
    "# 增大 CSV 字段大小限制\n",
    "csv.field_size_limit(10**7)\n",
    "\n",
    "# 输入和输出文件\n",
    "input_file = \"5_primer_data.csv\"\n",
    "output_file = \"6_primer_data.csv\"\n",
    "\n",
    "# 需要保留的列\n",
    "sequence_columns = [\n",
    "    \"ori_sequence\", \"F1_sequence\", \"F2_sequence\", \"F3_sequence\", \n",
    "    \"B1_sequence\", \"B2_sequence\", \"B3_sequence\", \"LB_sequence\", \"LF_sequence\"\n",
    "]\n",
    "\n",
    "# 处理文件\n",
    "data_to_save = []\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fields = reader.fieldnames\n",
    "\n",
    "    for row in reader:\n",
    "        # 条件过滤\n",
    "        if row.get(\"FIP_position\") and not row.get(\"F1_sequence\"):\n",
    "            continue  # 去掉 FIP_position 不为空但 F1_sequence 为空的行\n",
    "        if row.get(\"BIP_position\") and not row.get(\"B1_sequence\"):\n",
    "            continue  # 去掉 BIP_position 不为空但 B1_sequence 为空的行\n",
    "\n",
    "        # 只保留指定的列\n",
    "        filtered_row = {key: row[key] for key in sequence_columns if key in row}\n",
    "        data_to_save.append(filtered_row)\n",
    "\n",
    "# 保存处理后的数据\n",
    "with open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as outfile:\n",
    "    writer = csv.DictWriter(outfile, fieldnames=sequence_columns)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data_to_save)\n",
    "\n",
    "print(f\"数据已成功处理并保存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9d1af4-a75a-482d-9dec-ef29a6577fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#对其中四条引物序列做反向互补。\n",
    "\n",
    "import csv\n",
    "\n",
    "# 增大 CSV 字段大小限制\n",
    "csv.field_size_limit(10**7)\n",
    "\n",
    "# 输入和输出文件\n",
    "input_file = \"6_primer_data.csv\"\n",
    "output_file = \"7_primer_data.csv\"\n",
    "\n",
    "# 互补碱基映射\n",
    "complement = str.maketrans(\"ATCG\", \"TAGC\")\n",
    "\n",
    "# 生成反向重复序列函数\n",
    "def reverse_complement(sequence):\n",
    "    if not sequence:\n",
    "        return \"\"  # 空值处理\n",
    "    try:\n",
    "        return sequence.translate(complement)[::-1]  # 替换碱基并反转\n",
    "    except Exception as e:\n",
    "        print(f\"处理序列时出错: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 处理文件\n",
    "data_to_save = []\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fields = reader.fieldnames\n",
    "\n",
    "    # 新的列名映射\n",
    "    column_mapping = {\n",
    "        \"B3_sequence\": \"B3_reverse_sequence\",\n",
    "        \"LF_sequence\": \"LF_reverse_sequence\",\n",
    "        \"F1_sequence\": \"F1_reverse_sequence\",\n",
    "        \"B2_sequence\": \"B2_reverse_sequence\"\n",
    "    }\n",
    "\n",
    "    # 更新字段列表\n",
    "    updated_fields = [column_mapping.get(col, col) for col in fields]\n",
    "\n",
    "    for row in reader:\n",
    "        # 替换目标列为反向重复序列并更新列名\n",
    "        for old_col, new_col in column_mapping.items():\n",
    "            if old_col in row:\n",
    "                row[new_col] = reverse_complement(row.pop(old_col))\n",
    "        data_to_save.append(row)\n",
    "\n",
    "# 保存处理后的数据\n",
    "with open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as outfile:\n",
    "    writer = csv.DictWriter(outfile, fieldnames=updated_fields)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data_to_save)\n",
    "\n",
    "print(f\"数据已成功处理并保存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad2d29-023d-45fd-929e-a8a47ac47e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#对数据进行去重处理，去除八条引物完全相同的数据。\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 读取原始文件\n",
    "df = pd.read_csv('7_primer_data.csv')\n",
    "\n",
    "# 筛选出指定列\n",
    "columns_to_check = [\n",
    "    \"F1_reverse_sequence\", \"F2_sequence\", \"F3_sequence\", \"B1_sequence\", \n",
    "    \"B2_reverse_sequence\", \"B3_reverse_sequence\", \"LB_sequence\", \"LF_reverse_sequence\"\n",
    "]\n",
    "\n",
    "# 去重：删除这八列内容完全相同的行\n",
    "filtered_df = df.drop_duplicates(subset=columns_to_check)\n",
    "\n",
    "# 保存筛选后的数据到新的文件\n",
    "filtered_df.to_csv('7_primer_data_filtered.csv', index=False)\n",
    "\n",
    "# 打印新文件的行数\n",
    "print(f\"新文件共有 {len(filtered_df)} 行\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c455d91d-cdcd-43a3-8b07-7fde514588b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取引物段，并计算引物段长度，过滤掉引物段长度等于0并且大于300的数据。\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv('7_primer_data_filtered.csv')\n",
    "\n",
    "# 定义一个函数来提取引物段序列\n",
    "def extract_primer_segment(row):\n",
    "    ori_sequence = row['ori_sequence']\n",
    "    F3_sequence = row['F3_sequence']\n",
    "    B3_reverse_sequence = row['B3_reverse_sequence']\n",
    "    \n",
    "    # 使用正则表达式查找F3和B3之间的序列\n",
    "    pattern = re.escape(F3_sequence) + '(.*?)' + re.escape(B3_reverse_sequence)\n",
    "    match = re.search(pattern, ori_sequence)\n",
    "    \n",
    "    if match:\n",
    "        primer_segment = match.group(0)  # 包含F3和B3的序列\n",
    "        return primer_segment\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# 应用函数来提取引物段序列\n",
    "df['primer_segment'] = df.apply(extract_primer_segment, axis=1)\n",
    "\n",
    "# 计算引物段序列的长度\n",
    "df['primer_segment_length'] = df['primer_segment'].apply(lambda x: len(x) if x else 0)\n",
    "\n",
    "# 过滤掉引物段长度为0或大于300的行\n",
    "df_filtered = df[(df['primer_segment_length'] > 0) & (df['primer_segment_length'] <= 300)]\n",
    "\n",
    "# 将结果保存到新的CSV文件\n",
    "df_filtered.to_csv('8_primer_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ab24ec-6adb-4189-94f2-2f310b23d8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to 8_primer_data.csv\n"
     ]
    }
   ],
   "source": [
    "# 有一些数据比较顽皮，去掉\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 输入文件\n",
    "input_file = \"8_primer_data.csv\"\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "# 过滤数据：去掉 F2_sequence 列中内容为 \"GCAGCTTCTGGCTGACCAA\" 的行\n",
    "filtered_data = data[data['F2_sequence'] != \"GCAGCTTCTGGCTGACCAA\"]\n",
    "filtered_data = data[data['primer_segment'] != \"TACCCGGCCACCCTTGTCTACCGGACCTGTTGCCTCGGCGGGCCTGCAGCGATGCTGCCGGGGGAGCTTCTCCTCCCCGGGCCCGTGTCCGCCGGGGACACCGCAAGAACCGTCGGTGAACGATTGGCGTCTGAGCATGAGAGCGATAATAATCCAGTCAAAACTTTCAACAACGGATCTCTTGGTTCCGACAT\"]\n",
    "filtered_data = data[data['primer_segment'] != \"AGATGTATCTGAGGGTCTGTAGCTCAGTTGGTTAGAGCACACGCTTGATAAGCGTGGGGTCACAAGTTCAAGTCTTGTCAGACCCACCATGACTTTGACTGGTTGAAGTTATAGATAAAAGATACATGATTGATGATGTAAGCTGGGGACTTAGCTTAGTTGGTAGAGCGCCTGCTTTGCACGCAGGAGGTCAGGAGTTCGACTCTCCTAGTCTCCACCAGAA\"]\n",
    "filtered_data = data[data['primer_segment'] != \"CGCTGGCTGGCTTTTCTGCCACCGCGCTGACCAACCTCGTCGCGGAACCATTCGCTAAACTCGAACAGGACTTTGGCGGCTCCATCGGTGTGTACGCGATGGATACCGGCTCAGGCGCAACTGTAAGTTACCGCGCTGAGGAGCGCTTCCCACTGTGCAGCTCATTCAAGGGCTTTCTTGCTGCCGCTGTG\"]\n",
    "\n",
    "\n",
    "# 保存过滤后的数据到原文件\n",
    "filtered_data.to_csv(input_file, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to {input_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ac41217-a706-474d-a89a-b8e13af4caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv('8_primer_data.csv')\n",
    "\n",
    "# 随机划分数据集\n",
    "# 首先划分出测试集\n",
    "train_val, test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# 然后从训练集和验证集中划分出验证集\n",
    "train, val = train_test_split(train_val, test_size=0.1 / 0.9, random_state=42)\n",
    "\n",
    "# 保存划分后的数据集\n",
    "train.to_csv('9_train_data.csv', index=False)\n",
    "val.to_csv('9_val_data.csv', index=False)\n",
    "test.to_csv('9_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "747a6b9a-07a7-42a2-959b-505c1b2e75b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 67 due to duplicate primer sequences.\n",
      "Skipping row 160 due to duplicate primer sequences.\n",
      "Processed data saved to train_data_300.csv\n",
      "Processed data saved to val_data_300.csv\n",
      "Processed data saved to test_data_300.csv\n"
     ]
    }
   ],
   "source": [
    "#首先进行数据增强300长度。\n",
    "# 生成标签序列。\n",
    "#one-hot编码。\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# 文件路径\n",
    "input_files = [\"9_train_data.csv\", \"9_val_data.csv\", \"9_test_data.csv\"]\n",
    "output_files = [\"train_data_300.csv\", \"val_data_300.csv\", \"test_data_300.csv\"]\n",
    "\n",
    "# 序列与标签的对应规则\n",
    "primer_to_label = {\n",
    "    \"F3_sequence\": 4,\n",
    "    \"F2_sequence\": 3,\n",
    "    \"LF_reverse_sequence\": 2,\n",
    "    \"F1_reverse_sequence\": 1,\n",
    "    \"B1_sequence\": 5,\n",
    "    \"LB_sequence\": 6,\n",
    "    \"B2_reverse_sequence\": 7,\n",
    "    \"B3_reverse_sequence\": 8\n",
    "}\n",
    "\n",
    "# 碱基到 one-hot 编码的映射\n",
    "base_to_one_hot = {\n",
    "    'A': '1000',\n",
    "    'T': '0100',\n",
    "    'C': '0010',\n",
    "    'G': '0001'\n",
    "}\n",
    "\n",
    "# 生成标签序列逻辑\n",
    "def generate_label_sequence(row):\n",
    "    enhanced_sequence = row['enhanced_sequence']\n",
    "    label_sequence = [0] * 300  # 修改标签序列长度为300\n",
    "\n",
    "    for primer, label in primer_to_label.items():\n",
    "        primer_sequence = row.get(primer, \"\")\n",
    "        if primer_sequence and isinstance(primer_sequence, str) and primer_sequence in enhanced_sequence:\n",
    "            start = enhanced_sequence.index(primer_sequence)\n",
    "            end = start + len(primer_sequence)\n",
    "            for i in range(start, end):\n",
    "                label_sequence[i] = label\n",
    "\n",
    "    return ''.join(map(str, label_sequence))  # 转为字符串格式\n",
    "\n",
    "# 将 enhanced_sequence 转换为 one-hot 编码\n",
    "def convert_to_one_hot(sequence):\n",
    "    if not isinstance(sequence, str):\n",
    "        return ''  # 如果序列无效，返回空字符串\n",
    "\n",
    "    one_hot_sequence = []\n",
    "    for base in sequence:\n",
    "        one_hot_sequence.append(base_to_one_hot.get(base, '0000'))  # 非标准碱基填充 '0000'\n",
    "    return ''.join(one_hot_sequence)  # 连接为一个长字符串\n",
    "\n",
    "def find_all_occurrences(sequence, sub_sequence):\n",
    "    \"\"\"找到所有子序列的位置\"\"\"\n",
    "    positions = []\n",
    "    pos = -1\n",
    "    while True:\n",
    "        pos = sequence.find(sub_sequence, pos + 1)\n",
    "        if pos == -1:\n",
    "            break\n",
    "        positions.append(pos)\n",
    "    return positions\n",
    "\n",
    "# 滑动窗口增强和处理文件\n",
    "def process_file(input_file, output_file):\n",
    "    # 读取原始数据\n",
    "    data = pd.read_csv(input_file)\n",
    "    enhanced_data = []\n",
    "\n",
    "    # 遍历每一行数据\n",
    "    for index, row in data.iterrows():\n",
    "        ori_sequence = row['ori_sequence']\n",
    "        F3_sequence = row['F3_sequence']\n",
    "        B3_reverse_sequence = row['B3_reverse_sequence']\n",
    "\n",
    "        try:\n",
    "            # 检查是否存在重复的引物序列\n",
    "            f3_positions = find_all_occurrences(ori_sequence, F3_sequence)\n",
    "            b3_positions = find_all_occurrences(ori_sequence, B3_reverse_sequence)\n",
    "            \n",
    "            if len(f3_positions) > 1 or len(b3_positions) > 1:\n",
    "                print(f\"Skipping row {index} due to duplicate primer sequences.\")\n",
    "                continue\n",
    "                \n",
    "            if len(f3_positions) == 0 or len(b3_positions) == 0:\n",
    "                print(f\"Skipping row {index} due to missing primer sequences.\")\n",
    "                continue\n",
    "\n",
    "            start_A = f3_positions[0]\n",
    "            end_A = b3_positions[0] + len(B3_reverse_sequence)\n",
    "\n",
    "            # 如果序列 A 的长度大于 500bp，跳过该行\n",
    "            if end_A - start_A > 500:\n",
    "                print(f\"Skipping row {index} due to sequence A length > 500.\")\n",
    "                continue\n",
    "\n",
    "            # 初始化窗口\n",
    "            start_window = start_A\n",
    "            end_window = start_window + 300\n",
    "\n",
    "            # 如果窗口超出原始序列范围，调整起点\n",
    "            if end_window > len(ori_sequence):\n",
    "                start_window = max(0, len(ori_sequence) - 300)\n",
    "                end_window = start_window + 300\n",
    "\n",
    "            # 滑动窗口增强\n",
    "            while start_window >= 0 and end_window >= end_A:\n",
    "                # 检查窗口起点是否为负值\n",
    "                if start_window < 0:\n",
    "                    print(f\"Skipping window in row {index} due to negative start position.\")\n",
    "                    break\n",
    "                    \n",
    "                window_sequence = ori_sequence[start_window:end_window]\n",
    "                \n",
    "                # 检查窗口长度\n",
    "                if len(window_sequence) != 300:\n",
    "                    print(f\"Skipping window in row {index} due to incorrect window length.\")\n",
    "                    break\n",
    "\n",
    "                # 创建增强行\n",
    "                new_row = row.copy()\n",
    "                new_row['enhanced_sequence'] = window_sequence\n",
    "                new_row['label_sequence'] = generate_label_sequence(new_row)\n",
    "                new_row['one_hot_encoded'] = convert_to_one_hot(window_sequence)\n",
    "                new_row = new_row.drop(labels=['ori_sequence'])\n",
    "                enhanced_data.append(new_row)\n",
    "\n",
    "                # 滑动窗口向左移动 5bp\n",
    "                start_window -= 5\n",
    "                end_window = start_window + 300\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {index}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # 将增强数据转换为 DataFrame\n",
    "    enhanced_df = pd.DataFrame(enhanced_data)\n",
    "\n",
    "    # 统计每个 primer_segment 的数量，并补充不足的部分\n",
    "    primer_segment_counts = enhanced_df['primer_segment'].value_counts()\n",
    "\n",
    "    # 对于每个 primer_segment，检查是否有不足25条的情况\n",
    "    for primer_segment, count in primer_segment_counts.items():\n",
    "        if count < 25:\n",
    "            # 找到该 primer_segment 对应的所有行\n",
    "            rows_to_copy = enhanced_df[enhanced_df['primer_segment'] == primer_segment]\n",
    "            # 随机复制并补充到25条\n",
    "            additional_rows = rows_to_copy.sample(25 - count, replace=True)\n",
    "            enhanced_df = pd.concat([enhanced_df, additional_rows], ignore_index=True)\n",
    "\n",
    "    # 保存增强数据\n",
    "    enhanced_df.to_csv(output_file, index=False)\n",
    "    print(f\"Processed data saved to {output_file}\")\n",
    "\n",
    "# 对每个文件进行处理\n",
    "for input_file, output_file in zip(input_files, output_files):\n",
    "    process_file(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "507d2dea-de0b-491c-9766-b4cb833cce0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping window in row 26 due to incorrect window length.\n",
      "Skipping row 67 due to duplicate primer sequences.\n",
      "Skipping row 160 due to duplicate primer sequences.\n",
      "Processed data saved to train_data_500.csv\n",
      "Processed data saved to val_data_500.csv\n",
      "Skipping window in row 2 due to incorrect window length.\n",
      "Processed data saved to test_data_500.csv\n"
     ]
    }
   ],
   "source": [
    "#首先进行数据增强500长度。\n",
    "# 生成标签序列。\n",
    "#one-hot编码。\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# 文件路径\n",
    "input_files = [\"9_train_data.csv\", \"9_val_data.csv\", \"9_test_data.csv\"]\n",
    "output_files = [\"train_data_500.csv\", \"val_data_500.csv\", \"test_data_500.csv\"]\n",
    "\n",
    "# 序列与标签的对应规则\n",
    "primer_to_label = {\n",
    "    \"F3_sequence\": 4,\n",
    "    \"F2_sequence\": 3,\n",
    "    \"LF_reverse_sequence\": 2,\n",
    "    \"F1_reverse_sequence\": 1,\n",
    "    \"B1_sequence\": 5,\n",
    "    \"LB_sequence\": 6,\n",
    "    \"B2_reverse_sequence\": 7,\n",
    "    \"B3_reverse_sequence\": 8\n",
    "}\n",
    "\n",
    "# 碱基到 one-hot 编码的映射\n",
    "base_to_one_hot = {\n",
    "    'A': '1000',\n",
    "    'T': '0100',\n",
    "    'C': '0010',\n",
    "    'G': '0001'\n",
    "}\n",
    "\n",
    "# 生成标签序列逻辑\n",
    "def generate_label_sequence(row):\n",
    "    enhanced_sequence = row['enhanced_sequence']\n",
    "    label_sequence = [0] * 500  # 修改标签序列长度为500\n",
    "\n",
    "    for primer, label in primer_to_label.items():\n",
    "        primer_sequence = row.get(primer, \"\")\n",
    "        if primer_sequence and isinstance(primer_sequence, str) and primer_sequence in enhanced_sequence:\n",
    "            start = enhanced_sequence.index(primer_sequence)\n",
    "            end = start + len(primer_sequence)\n",
    "            for i in range(start, end):\n",
    "                label_sequence[i] = label\n",
    "\n",
    "    return ''.join(map(str, label_sequence))  # 转为字符串格式\n",
    "\n",
    "# 将 enhanced_sequence 转换为 one-hot 编码\n",
    "def convert_to_one_hot(sequence):\n",
    "    if not isinstance(sequence, str):\n",
    "        return ''  # 如果序列无效，返回空字符串\n",
    "\n",
    "    one_hot_sequence = []\n",
    "    for base in sequence:\n",
    "        one_hot_sequence.append(base_to_one_hot.get(base, '0000'))  # 非标准碱基填充 '0000'\n",
    "    return ''.join(one_hot_sequence)  # 连接为一个长字符串\n",
    "\n",
    "def find_all_occurrences(sequence, sub_sequence):\n",
    "    \"\"\"找到所有子序列的位置\"\"\"\n",
    "    positions = []\n",
    "    pos = -1\n",
    "    while True:\n",
    "        pos = sequence.find(sub_sequence, pos + 1)\n",
    "        if pos == -1:\n",
    "            break\n",
    "        positions.append(pos)\n",
    "    return positions\n",
    "\n",
    "# 滑动窗口增强和处理文件\n",
    "def process_file(input_file, output_file):\n",
    "    # 读取原始数据\n",
    "    data = pd.read_csv(input_file)\n",
    "    enhanced_data = []\n",
    "\n",
    "    # 遍历每一行数据\n",
    "    for index, row in data.iterrows():\n",
    "        ori_sequence = row['ori_sequence']\n",
    "        F3_sequence = row['F3_sequence']\n",
    "        B3_reverse_sequence = row['B3_reverse_sequence']\n",
    "\n",
    "        try:\n",
    "            # 检查是否存在重复的引物序列\n",
    "            f3_positions = find_all_occurrences(ori_sequence, F3_sequence)\n",
    "            b3_positions = find_all_occurrences(ori_sequence, B3_reverse_sequence)\n",
    "            \n",
    "            if len(f3_positions) > 1 or len(b3_positions) > 1:\n",
    "                print(f\"Skipping row {index} due to duplicate primer sequences.\")\n",
    "                continue\n",
    "                \n",
    "            if len(f3_positions) == 0 or len(b3_positions) == 0:\n",
    "                print(f\"Skipping row {index} due to missing primer sequences.\")\n",
    "                continue\n",
    "\n",
    "            start_A = f3_positions[0]\n",
    "            end_A = b3_positions[0] + len(B3_reverse_sequence)\n",
    "\n",
    "            # 如果序列 A 的长度大于 500bp，跳过该行\n",
    "            if end_A - start_A > 500:\n",
    "                print(f\"Skipping row {index} due to sequence A length > 500.\")\n",
    "                continue\n",
    "\n",
    "            # 初始化窗口\n",
    "            start_window = start_A\n",
    "            end_window = start_window + 500  # 修改窗口大小为500\n",
    "\n",
    "            # 如果窗口超出原始序列范围，调整起点\n",
    "            if end_window > len(ori_sequence):\n",
    "                start_window = max(0, len(ori_sequence) - 500)  # 修改为500\n",
    "                end_window = start_window + 500  # 修改为500\n",
    "\n",
    "            # 滑动窗口增强\n",
    "            while start_window >= 0 and end_window >= end_A:\n",
    "                # 检查窗口起点是否为负值\n",
    "                if start_window < 0:\n",
    "                    print(f\"Skipping window in row {index} due to negative start position.\")\n",
    "                    break\n",
    "                    \n",
    "                window_sequence = ori_sequence[start_window:end_window]\n",
    "                \n",
    "                # 检查窗口长度\n",
    "                if len(window_sequence) != 500:  # 修改检查长度为500\n",
    "                    print(f\"Skipping window in row {index} due to incorrect window length.\")\n",
    "                    break\n",
    "\n",
    "                # 创建增强行\n",
    "                new_row = row.copy()\n",
    "                new_row['enhanced_sequence'] = window_sequence\n",
    "                new_row['label_sequence'] = generate_label_sequence(new_row)\n",
    "                new_row['one_hot_encoded'] = convert_to_one_hot(window_sequence)\n",
    "                new_row = new_row.drop(labels=['ori_sequence'])\n",
    "                enhanced_data.append(new_row)\n",
    "\n",
    "                # 滑动窗口向左移动 10bp\n",
    "                start_window -= 10  # 修改步长为10bp\n",
    "                end_window = start_window + 500  # 修改为500\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {index}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # 将增强数据转换为 DataFrame\n",
    "    enhanced_df = pd.DataFrame(enhanced_data)\n",
    "\n",
    "    # 统计每个 primer_segment 的数量，并补充不足的部分\n",
    "    primer_segment_counts = enhanced_df['primer_segment'].value_counts()\n",
    "\n",
    "    # 对于每个 primer_segment，检查是否有不足25条的情况\n",
    "    for primer_segment, count in primer_segment_counts.items():\n",
    "        if count < 25:\n",
    "            # 找到该 primer_segment 对应的所有行\n",
    "            rows_to_copy = enhanced_df[enhanced_df['primer_segment'] == primer_segment]\n",
    "            # 随机复制并补充到25条\n",
    "            additional_rows = rows_to_copy.sample(25 - count, replace=True)\n",
    "            enhanced_df = pd.concat([enhanced_df, additional_rows], ignore_index=True)\n",
    "\n",
    "    # 保存增强数据\n",
    "    enhanced_df.to_csv(output_file, index=False)\n",
    "    print(f\"Processed data saved to {output_file}\")\n",
    "\n",
    "# 对每个文件进行处理\n",
    "for input_file, output_file in zip(input_files, output_files):\n",
    "    process_file(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d1fcf89-fc6a-4f4c-afbc-59629cabd15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有文件已更新。\n"
     ]
    }
   ],
   "source": [
    "#去除标签序列不合理的数据，每一格标签必须包含431578\n",
    "import pandas as pd\n",
    "\n",
    "# 定义一个函数来处理每个 CSV 文件\n",
    "def process_file(file_path):\n",
    "    # 读取 CSV 文件\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 定义需要保留的数字\n",
    "    required_digits = {'4', '3', '1', '5', '7', '8'}\n",
    "    \n",
    "    # 使用 apply 来判断每一行的 'label_sequence' 是否包含所有需要的数字\n",
    "    def contains_all_required_digits(label_sequence):\n",
    "        return all(digit in label_sequence for digit in required_digits)\n",
    "    \n",
    "    # 过滤掉不包含所有需要的数字的行\n",
    "    filtered_df = df[df['label_sequence'].apply(contains_all_required_digits)]\n",
    "    \n",
    "    # 直接保存回原文件\n",
    "    filtered_df.to_csv(file_path, index=False)\n",
    "\n",
    "# 列出所有需要处理的文件路径\n",
    "file_paths = [\n",
    "    'test_data_500.csv',\n",
    "    'train_data_300.csv',\n",
    "    'train_data_500.csv',\n",
    "    'val_data_300.csv',\n",
    "    'val_data_500.csv',\n",
    "    'test_data_300.csv'\n",
    "]\n",
    "\n",
    "# 对每个文件进行处理\n",
    "for file_path in file_paths:\n",
    "    process_file(file_path)\n",
    "\n",
    "print(\"所有文件已更新。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b19a0-015e-4d34-a1b8-7f8243ee0758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
