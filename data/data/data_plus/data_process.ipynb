{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82693711-a45a-4b63-af2a-e67d0470c9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件合并完成，已保存为 'merged_file.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取两个 CSV 文件\n",
    "file1 = 'data_cn.csv'\n",
    "file2 = 'data_lpb.csv'\n",
    "\n",
    "df1 = pd.read_csv(file1)\n",
    "df2 = pd.read_csv(file2)\n",
    "\n",
    "# 合并这两个 DataFrame\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# 保存合并后的结果为新的 CSV 文件\n",
    "merged_df.to_csv('data_plus.csv', index=False)\n",
    "\n",
    "print(\"文件合并完成，已保存为 'merged_file.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14655e9-fbd5-44ad-94d8-0ab3b1afc4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新文件共有 469 行\n"
     ]
    }
   ],
   "source": [
    "#对数据进行去重处理，去除八条引物完全相同的数据。\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 读取原始文件\n",
    "df = pd.read_csv('data_plus.csv')\n",
    "\n",
    "# 筛选出指定列\n",
    "columns_to_check = [\n",
    "    \"F1_reverse_sequence\", \"F2_sequence\", \"F3_sequence\", \"B1_sequence\", \n",
    "    \"B2_reverse_sequence\", \"B3_reverse_sequence\", \"LB_sequence\", \"LF_reverse_sequence\"\n",
    "]\n",
    "\n",
    "# 去重：删除这八列内容完全相同的行\n",
    "filtered_df = df.drop_duplicates(subset=columns_to_check)\n",
    "\n",
    "# 保存筛选后的数据到新的文件\n",
    "filtered_df.to_csv('7_primer_data_filtered.csv', index=False)\n",
    "\n",
    "# 打印新文件的行数\n",
    "print(f\"新文件共有 {len(filtered_df)} 行\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb886e6-5b5e-4444-b39b-1929088eb146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "长度计算完成，已保存为 '7_primer_data_filtered.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取 CSV 文件\n",
    "file_path = '7_primer_data_filtered.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 计算 ori_sequence 列中每个序列的长度\n",
    "df['ori_sequence_length'] = df['ori_sequence'].apply(len)\n",
    "\n",
    "# 保存更新后的 DataFrame 为新的 CSV 文件\n",
    "df.to_csv('7_primer_data_filtered.csv', index=False)\n",
    "\n",
    "print(\"长度计算完成，已保存为 '7_primer_data_filtered.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c8116a7-7cc8-4053-8bb2-3545328e0e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取引物段，并计算引物段长度，过滤掉引物段长度等于0并且大于300的数据。\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv('7_primer_data_filtered.csv')\n",
    "\n",
    "# 定义一个函数来提取引物段序列\n",
    "def extract_primer_segment(row):\n",
    "    ori_sequence = row['ori_sequence']\n",
    "    F3_sequence = row['F3_sequence']\n",
    "    B3_reverse_sequence = row['B3_reverse_sequence']\n",
    "    \n",
    "    # 使用正则表达式查找F3和B3之间的序列\n",
    "    pattern = re.escape(F3_sequence) + '(.*?)' + re.escape(B3_reverse_sequence)\n",
    "    match = re.search(pattern, ori_sequence)\n",
    "    \n",
    "    if match:\n",
    "        primer_segment = match.group(0)  # 包含F3和B3的序列\n",
    "        return primer_segment\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# 应用函数来提取引物段序列\n",
    "df['primer_segment'] = df.apply(extract_primer_segment, axis=1)\n",
    "\n",
    "# 计算引物段序列的长度\n",
    "df['primer_segment_length'] = df['primer_segment'].apply(lambda x: len(x) if x else 0)\n",
    "\n",
    "# 过滤掉引物段长度为0或大于300的行\n",
    "df_filtered = df[(df['primer_segment_length'] > 0) & (df['primer_segment_length'] <= 300)]\n",
    "\n",
    "# 将结果保存到新的CSV文件\n",
    "df_filtered.to_csv('8_primer_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "230dfb69-ea7c-4fe5-8937-809f21efbc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv('8_primer_data.csv')\n",
    "\n",
    "# 随机划分数据集\n",
    "# 首先划分出测试集\n",
    "train_val, test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# 然后从训练集和验证集中划分出验证集\n",
    "train, val = train_test_split(train_val, test_size=0.1 / 0.9, random_state=42)\n",
    "\n",
    "# 保存划分后的数据集\n",
    "train.to_csv('9_train_data.csv', index=False)\n",
    "val.to_csv('9_val_data.csv', index=False)\n",
    "test.to_csv('9_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35b9a77d-0f5c-4f37-9d35-1bc06e0a2be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping window in row 1 due to incorrect window length.\n",
      "Skipping window in row 5 due to incorrect window length.\n",
      "Skipping row 9 due to duplicate primer sequences.\n",
      "Skipping window in row 12 due to incorrect window length.\n",
      "Skipping window in row 14 due to incorrect window length.\n",
      "Skipping window in row 23 due to incorrect window length.\n",
      "Skipping window in row 35 due to incorrect window length.\n",
      "Skipping window in row 36 due to incorrect window length.\n",
      "Skipping window in row 51 due to incorrect window length.\n",
      "Skipping window in row 52 due to incorrect window length.\n",
      "Skipping window in row 93 due to incorrect window length.\n",
      "Skipping window in row 114 due to incorrect window length.\n",
      "Skipping row 117 due to duplicate primer sequences.\n",
      "Skipping window in row 124 due to incorrect window length.\n",
      "Skipping window in row 126 due to incorrect window length.\n",
      "Skipping window in row 137 due to incorrect window length.\n",
      "Skipping window in row 141 due to incorrect window length.\n",
      "Skipping window in row 146 due to incorrect window length.\n",
      "Skipping window in row 147 due to incorrect window length.\n",
      "Skipping window in row 151 due to incorrect window length.\n",
      "Skipping window in row 170 due to incorrect window length.\n",
      "Skipping window in row 199 due to incorrect window length.\n",
      "Skipping window in row 200 due to incorrect window length.\n",
      "Skipping window in row 227 due to incorrect window length.\n",
      "Skipping window in row 238 due to incorrect window length.\n",
      "Skipping window in row 245 due to incorrect window length.\n",
      "Skipping window in row 259 due to incorrect window length.\n",
      "Skipping window in row 260 due to incorrect window length.\n",
      "Skipping window in row 283 due to incorrect window length.\n",
      "Skipping window in row 298 due to incorrect window length.\n",
      "Skipping window in row 311 due to incorrect window length.\n",
      "Skipping window in row 336 due to incorrect window length.\n",
      "Skipping row 338 due to duplicate primer sequences.\n",
      "Skipping window in row 340 due to incorrect window length.\n",
      "Skipping window in row 341 due to incorrect window length.\n",
      "Processed data saved to train_data_300.csv\n",
      "Skipping window in row 1 due to incorrect window length.\n",
      "Skipping window in row 8 due to incorrect window length.\n",
      "Skipping window in row 14 due to incorrect window length.\n",
      "Skipping window in row 25 due to incorrect window length.\n",
      "Skipping window in row 32 due to incorrect window length.\n",
      "Skipping window in row 34 due to incorrect window length.\n",
      "Processed data saved to val_data_300.csv\n",
      "Skipping window in row 2 due to incorrect window length.\n",
      "Skipping window in row 9 due to incorrect window length.\n",
      "Processed data saved to test_data_300.csv\n"
     ]
    }
   ],
   "source": [
    "#首先进行数据增强300长度。\n",
    "# 生成标签序列。\n",
    "#one-hot编码。\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# 文件路径\n",
    "input_files = [\"9_train_data.csv\", \"9_val_data.csv\", \"9_test_data.csv\"]\n",
    "output_files = [\"train_data_300.csv\", \"val_data_300.csv\", \"test_data_300.csv\"]\n",
    "\n",
    "# 序列与标签的对应规则\n",
    "primer_to_label = {\n",
    "    \"F3_sequence\": 4,\n",
    "    \"F2_sequence\": 3,\n",
    "    \"LF_reverse_sequence\": 2,\n",
    "    \"F1_reverse_sequence\": 1,\n",
    "    \"B1_sequence\": 5,\n",
    "    \"LB_sequence\": 6,\n",
    "    \"B2_reverse_sequence\": 7,\n",
    "    \"B3_reverse_sequence\": 8\n",
    "}\n",
    "\n",
    "# 碱基到 one-hot 编码的映射\n",
    "base_to_one_hot = {\n",
    "    'A': '1000',\n",
    "    'T': '0100',\n",
    "    'C': '0010',\n",
    "    'G': '0001'\n",
    "}\n",
    "\n",
    "# 生成标签序列逻辑\n",
    "def generate_label_sequence(row):\n",
    "    enhanced_sequence = row['enhanced_sequence']\n",
    "    label_sequence = [0] * 300  # 修改标签序列长度为300\n",
    "\n",
    "    for primer, label in primer_to_label.items():\n",
    "        primer_sequence = row.get(primer, \"\")\n",
    "        if primer_sequence and isinstance(primer_sequence, str) and primer_sequence in enhanced_sequence:\n",
    "            start = enhanced_sequence.index(primer_sequence)\n",
    "            end = start + len(primer_sequence)\n",
    "            for i in range(start, end):\n",
    "                label_sequence[i] = label\n",
    "\n",
    "    return ''.join(map(str, label_sequence))  # 转为字符串格式\n",
    "\n",
    "# 将 enhanced_sequence 转换为 one-hot 编码\n",
    "def convert_to_one_hot(sequence):\n",
    "    if not isinstance(sequence, str):\n",
    "        return ''  # 如果序列无效，返回空字符串\n",
    "\n",
    "    one_hot_sequence = []\n",
    "    for base in sequence:\n",
    "        one_hot_sequence.append(base_to_one_hot.get(base, '0000'))  # 非标准碱基填充 '0000'\n",
    "    return ''.join(one_hot_sequence)  # 连接为一个长字符串\n",
    "\n",
    "def find_all_occurrences(sequence, sub_sequence):\n",
    "    \"\"\"找到所有子序列的位置\"\"\"\n",
    "    positions = []\n",
    "    pos = -1\n",
    "    while True:\n",
    "        pos = sequence.find(sub_sequence, pos + 1)\n",
    "        if pos == -1:\n",
    "            break\n",
    "        positions.append(pos)\n",
    "    return positions\n",
    "\n",
    "# 滑动窗口增强和处理文件\n",
    "def process_file(input_file, output_file):\n",
    "    # 读取原始数据\n",
    "    data = pd.read_csv(input_file)\n",
    "    enhanced_data = []\n",
    "\n",
    "    # 遍历每一行数据\n",
    "    for index, row in data.iterrows():\n",
    "        ori_sequence = row['ori_sequence']\n",
    "        F3_sequence = row['F3_sequence']\n",
    "        B3_reverse_sequence = row['B3_reverse_sequence']\n",
    "\n",
    "        try:\n",
    "            # 检查是否存在重复的引物序列\n",
    "            f3_positions = find_all_occurrences(ori_sequence, F3_sequence)\n",
    "            b3_positions = find_all_occurrences(ori_sequence, B3_reverse_sequence)\n",
    "            \n",
    "            if len(f3_positions) > 1 or len(b3_positions) > 1:\n",
    "                print(f\"Skipping row {index} due to duplicate primer sequences.\")\n",
    "                continue\n",
    "                \n",
    "            if len(f3_positions) == 0 or len(b3_positions) == 0:\n",
    "                print(f\"Skipping row {index} due to missing primer sequences.\")\n",
    "                continue\n",
    "\n",
    "            start_A = f3_positions[0]\n",
    "            end_A = b3_positions[0] + len(B3_reverse_sequence)\n",
    "\n",
    "            # 如果序列 A 的长度大于 500bp，跳过该行\n",
    "            if end_A - start_A > 500:\n",
    "                print(f\"Skipping row {index} due to sequence A length > 500.\")\n",
    "                continue\n",
    "\n",
    "            # 初始化窗口\n",
    "            start_window = start_A\n",
    "            end_window = start_window + 300\n",
    "\n",
    "            # 如果窗口超出原始序列范围，调整起点\n",
    "            if end_window > len(ori_sequence):\n",
    "                start_window = max(0, len(ori_sequence) - 300)\n",
    "                end_window = start_window + 300\n",
    "\n",
    "            # 滑动窗口增强\n",
    "            while start_window >= 0 and end_window >= end_A:\n",
    "                # 检查窗口起点是否为负值\n",
    "                if start_window < 0:\n",
    "                    print(f\"Skipping window in row {index} due to negative start position.\")\n",
    "                    break\n",
    "                    \n",
    "                window_sequence = ori_sequence[start_window:end_window]\n",
    "                \n",
    "                # 检查窗口长度\n",
    "                if len(window_sequence) != 300:\n",
    "                    print(f\"Skipping window in row {index} due to incorrect window length.\")\n",
    "                    break\n",
    "\n",
    "                # 创建增强行\n",
    "                new_row = row.copy()\n",
    "                new_row['enhanced_sequence'] = window_sequence\n",
    "                new_row['label_sequence'] = generate_label_sequence(new_row)\n",
    "                new_row['one_hot_encoded'] = convert_to_one_hot(window_sequence)\n",
    "                new_row = new_row.drop(labels=['ori_sequence'])\n",
    "                enhanced_data.append(new_row)\n",
    "\n",
    "                # 滑动窗口向左移动 5bp\n",
    "                start_window -= 5\n",
    "                end_window = start_window + 300\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {index}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # 将增强数据转换为 DataFrame\n",
    "    enhanced_df = pd.DataFrame(enhanced_data)\n",
    "\n",
    "    # 统计每个 primer_segment 的数量，并补充不足的部分\n",
    "    primer_segment_counts = enhanced_df['primer_segment'].value_counts()\n",
    "\n",
    "    # 对于每个 primer_segment，检查是否有不足25条的情况\n",
    "    for primer_segment, count in primer_segment_counts.items():\n",
    "        if count < 25:\n",
    "            # 找到该 primer_segment 对应的所有行\n",
    "            rows_to_copy = enhanced_df[enhanced_df['primer_segment'] == primer_segment]\n",
    "            # 随机复制并补充到25条\n",
    "            additional_rows = rows_to_copy.sample(25 - count, replace=True)\n",
    "            enhanced_df = pd.concat([enhanced_df, additional_rows], ignore_index=True)\n",
    "\n",
    "    # 保存增强数据\n",
    "    enhanced_df.to_csv(output_file, index=False)\n",
    "    print(f\"Processed data saved to {output_file}\")\n",
    "\n",
    "# 对每个文件进行处理\n",
    "for input_file, output_file in zip(input_files, output_files):\n",
    "    process_file(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06fc3066-e4ec-459f-9e4d-c5666e959741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping window in row 1 due to incorrect window length.\n",
      "Skipping window in row 5 due to incorrect window length.\n",
      "Skipping row 9 due to duplicate primer sequences.\n",
      "Skipping window in row 11 due to incorrect window length.\n",
      "Skipping window in row 12 due to incorrect window length.\n",
      "Skipping window in row 14 due to incorrect window length.\n",
      "Skipping window in row 18 due to incorrect window length.\n",
      "Skipping window in row 19 due to incorrect window length.\n",
      "Skipping window in row 23 due to incorrect window length.\n",
      "Skipping window in row 34 due to incorrect window length.\n",
      "Skipping window in row 35 due to incorrect window length.\n",
      "Skipping window in row 36 due to incorrect window length.\n",
      "Skipping window in row 51 due to incorrect window length.\n",
      "Skipping window in row 52 due to incorrect window length.\n",
      "Skipping window in row 54 due to incorrect window length.\n",
      "Skipping window in row 72 due to incorrect window length.\n",
      "Skipping window in row 75 due to incorrect window length.\n",
      "Skipping window in row 93 due to incorrect window length.\n",
      "Skipping window in row 94 due to incorrect window length.\n",
      "Skipping window in row 114 due to incorrect window length.\n",
      "Skipping row 117 due to duplicate primer sequences.\n",
      "Skipping window in row 124 due to incorrect window length.\n",
      "Skipping window in row 126 due to incorrect window length.\n",
      "Skipping window in row 137 due to incorrect window length.\n",
      "Skipping window in row 141 due to incorrect window length.\n",
      "Skipping window in row 145 due to incorrect window length.\n",
      "Skipping window in row 146 due to incorrect window length.\n",
      "Skipping window in row 147 due to incorrect window length.\n",
      "Skipping window in row 151 due to incorrect window length.\n",
      "Skipping window in row 155 due to incorrect window length.\n",
      "Skipping window in row 163 due to incorrect window length.\n",
      "Skipping window in row 164 due to incorrect window length.\n",
      "Skipping window in row 167 due to incorrect window length.\n",
      "Skipping window in row 170 due to incorrect window length.\n",
      "Skipping window in row 175 due to incorrect window length.\n",
      "Skipping window in row 196 due to incorrect window length.\n",
      "Skipping window in row 199 due to incorrect window length.\n",
      "Skipping window in row 200 due to incorrect window length.\n",
      "Skipping window in row 227 due to incorrect window length.\n",
      "Skipping window in row 235 due to incorrect window length.\n",
      "Skipping window in row 238 due to incorrect window length.\n",
      "Skipping window in row 241 due to incorrect window length.\n",
      "Skipping window in row 245 due to incorrect window length.\n",
      "Skipping window in row 259 due to incorrect window length.\n",
      "Skipping window in row 260 due to incorrect window length.\n",
      "Skipping window in row 261 due to incorrect window length.\n",
      "Skipping window in row 283 due to incorrect window length.\n",
      "Skipping window in row 284 due to incorrect window length.\n",
      "Skipping window in row 286 due to incorrect window length.\n",
      "Skipping window in row 298 due to incorrect window length.\n",
      "Skipping window in row 302 due to incorrect window length.\n",
      "Skipping window in row 303 due to incorrect window length.\n",
      "Skipping window in row 311 due to incorrect window length.\n",
      "Skipping window in row 318 due to incorrect window length.\n",
      "Skipping window in row 319 due to incorrect window length.\n",
      "Skipping window in row 336 due to incorrect window length.\n",
      "Skipping window in row 337 due to incorrect window length.\n",
      "Skipping row 338 due to duplicate primer sequences.\n",
      "Skipping window in row 340 due to incorrect window length.\n",
      "Skipping window in row 341 due to incorrect window length.\n",
      "Processed data saved to train_data_500.csv\n",
      "Skipping window in row 1 due to incorrect window length.\n",
      "Skipping window in row 8 due to incorrect window length.\n",
      "Skipping window in row 13 due to incorrect window length.\n",
      "Skipping window in row 14 due to incorrect window length.\n",
      "Skipping window in row 17 due to incorrect window length.\n",
      "Skipping window in row 25 due to incorrect window length.\n",
      "Skipping window in row 32 due to incorrect window length.\n",
      "Skipping window in row 34 due to incorrect window length.\n",
      "Processed data saved to val_data_500.csv\n",
      "Skipping window in row 1 due to incorrect window length.\n",
      "Skipping window in row 2 due to incorrect window length.\n",
      "Skipping window in row 3 due to incorrect window length.\n",
      "Skipping window in row 8 due to incorrect window length.\n",
      "Skipping window in row 9 due to incorrect window length.\n",
      "Processed data saved to test_data_500.csv\n"
     ]
    }
   ],
   "source": [
    "#首先进行数据增强500长度。\n",
    "# 生成标签序列。\n",
    "#one-hot编码。\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# 文件路径\n",
    "input_files = [\"9_train_data.csv\", \"9_val_data.csv\", \"9_test_data.csv\"]\n",
    "output_files = [\"train_data_500.csv\", \"val_data_500.csv\", \"test_data_500.csv\"]\n",
    "\n",
    "# 序列与标签的对应规则\n",
    "primer_to_label = {\n",
    "    \"F3_sequence\": 4,\n",
    "    \"F2_sequence\": 3,\n",
    "    \"LF_reverse_sequence\": 2,\n",
    "    \"F1_reverse_sequence\": 1,\n",
    "    \"B1_sequence\": 5,\n",
    "    \"LB_sequence\": 6,\n",
    "    \"B2_reverse_sequence\": 7,\n",
    "    \"B3_reverse_sequence\": 8\n",
    "}\n",
    "\n",
    "# 碱基到 one-hot 编码的映射\n",
    "base_to_one_hot = {\n",
    "    'A': '1000',\n",
    "    'T': '0100',\n",
    "    'C': '0010',\n",
    "    'G': '0001'\n",
    "}\n",
    "\n",
    "# 生成标签序列逻辑\n",
    "def generate_label_sequence(row):\n",
    "    enhanced_sequence = row['enhanced_sequence']\n",
    "    label_sequence = [0] * 500  # 修改标签序列长度为500\n",
    "\n",
    "    for primer, label in primer_to_label.items():\n",
    "        primer_sequence = row.get(primer, \"\")\n",
    "        if primer_sequence and isinstance(primer_sequence, str) and primer_sequence in enhanced_sequence:\n",
    "            start = enhanced_sequence.index(primer_sequence)\n",
    "            end = start + len(primer_sequence)\n",
    "            for i in range(start, end):\n",
    "                label_sequence[i] = label\n",
    "\n",
    "    return ''.join(map(str, label_sequence))  # 转为字符串格式\n",
    "\n",
    "# 将 enhanced_sequence 转换为 one-hot 编码\n",
    "def convert_to_one_hot(sequence):\n",
    "    if not isinstance(sequence, str):\n",
    "        return ''  # 如果序列无效，返回空字符串\n",
    "\n",
    "    one_hot_sequence = []\n",
    "    for base in sequence:\n",
    "        one_hot_sequence.append(base_to_one_hot.get(base, '0000'))  # 非标准碱基填充 '0000'\n",
    "    return ''.join(one_hot_sequence)  # 连接为一个长字符串\n",
    "\n",
    "def find_all_occurrences(sequence, sub_sequence):\n",
    "    \"\"\"找到所有子序列的位置\"\"\"\n",
    "    positions = []\n",
    "    pos = -1\n",
    "    while True:\n",
    "        pos = sequence.find(sub_sequence, pos + 1)\n",
    "        if pos == -1:\n",
    "            break\n",
    "        positions.append(pos)\n",
    "    return positions\n",
    "\n",
    "# 滑动窗口增强和处理文件\n",
    "def process_file(input_file, output_file):\n",
    "    # 读取原始数据\n",
    "    data = pd.read_csv(input_file)\n",
    "    enhanced_data = []\n",
    "\n",
    "    # 遍历每一行数据\n",
    "    for index, row in data.iterrows():\n",
    "        ori_sequence = row['ori_sequence']\n",
    "        F3_sequence = row['F3_sequence']\n",
    "        B3_reverse_sequence = row['B3_reverse_sequence']\n",
    "\n",
    "        try:\n",
    "            # 检查是否存在重复的引物序列\n",
    "            f3_positions = find_all_occurrences(ori_sequence, F3_sequence)\n",
    "            b3_positions = find_all_occurrences(ori_sequence, B3_reverse_sequence)\n",
    "            \n",
    "            if len(f3_positions) > 1 or len(b3_positions) > 1:\n",
    "                print(f\"Skipping row {index} due to duplicate primer sequences.\")\n",
    "                continue\n",
    "                \n",
    "            if len(f3_positions) == 0 or len(b3_positions) == 0:\n",
    "                print(f\"Skipping row {index} due to missing primer sequences.\")\n",
    "                continue\n",
    "\n",
    "            start_A = f3_positions[0]\n",
    "            end_A = b3_positions[0] + len(B3_reverse_sequence)\n",
    "\n",
    "            # 如果序列 A 的长度大于 500bp，跳过该行\n",
    "            if end_A - start_A > 500:\n",
    "                print(f\"Skipping row {index} due to sequence A length > 500.\")\n",
    "                continue\n",
    "\n",
    "            # 初始化窗口\n",
    "            start_window = start_A\n",
    "            end_window = start_window + 500  # 修改窗口大小为500\n",
    "\n",
    "            # 如果窗口超出原始序列范围，调整起点\n",
    "            if end_window > len(ori_sequence):\n",
    "                start_window = max(0, len(ori_sequence) - 500)  # 修改为500\n",
    "                end_window = start_window + 500  # 修改为500\n",
    "\n",
    "            # 滑动窗口增强\n",
    "            while start_window >= 0 and end_window >= end_A:\n",
    "                # 检查窗口起点是否为负值\n",
    "                if start_window < 0:\n",
    "                    print(f\"Skipping window in row {index} due to negative start position.\")\n",
    "                    break\n",
    "                    \n",
    "                window_sequence = ori_sequence[start_window:end_window]\n",
    "                \n",
    "                # 检查窗口长度\n",
    "                if len(window_sequence) != 500:  # 修改检查长度为500\n",
    "                    print(f\"Skipping window in row {index} due to incorrect window length.\")\n",
    "                    break\n",
    "\n",
    "                # 创建增强行\n",
    "                new_row = row.copy()\n",
    "                new_row['enhanced_sequence'] = window_sequence\n",
    "                new_row['label_sequence'] = generate_label_sequence(new_row)\n",
    "                new_row['one_hot_encoded'] = convert_to_one_hot(window_sequence)\n",
    "                new_row = new_row.drop(labels=['ori_sequence'])\n",
    "                enhanced_data.append(new_row)\n",
    "\n",
    "                # 滑动窗口向左移动 10bp\n",
    "                start_window -= 10  # 修改步长为10bp\n",
    "                end_window = start_window + 500  # 修改为500\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {index}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # 将增强数据转换为 DataFrame\n",
    "    enhanced_df = pd.DataFrame(enhanced_data)\n",
    "\n",
    "    # 统计每个 primer_segment 的数量，并补充不足的部分\n",
    "    primer_segment_counts = enhanced_df['primer_segment'].value_counts()\n",
    "\n",
    "    # 对于每个 primer_segment，检查是否有不足25条的情况\n",
    "    for primer_segment, count in primer_segment_counts.items():\n",
    "        if count < 25:\n",
    "            # 找到该 primer_segment 对应的所有行\n",
    "            rows_to_copy = enhanced_df[enhanced_df['primer_segment'] == primer_segment]\n",
    "            # 随机复制并补充到25条\n",
    "            additional_rows = rows_to_copy.sample(25 - count, replace=True)\n",
    "            enhanced_df = pd.concat([enhanced_df, additional_rows], ignore_index=True)\n",
    "\n",
    "    # 保存增强数据\n",
    "    enhanced_df.to_csv(output_file, index=False)\n",
    "    print(f\"Processed data saved to {output_file}\")\n",
    "\n",
    "# 对每个文件进行处理\n",
    "for input_file, output_file in zip(input_files, output_files):\n",
    "    process_file(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c414dac-5980-4d49-8b1c-4f452634548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有文件已更新。\n"
     ]
    }
   ],
   "source": [
    "#去除标签序列不合理的数据，每一格标签必须包含431578\n",
    "import pandas as pd\n",
    "\n",
    "# 定义一个函数来处理每个 CSV 文件\n",
    "def process_file(file_path):\n",
    "    # 读取 CSV 文件\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 定义需要保留的数字\n",
    "    required_digits = {'4', '3', '1', '5', '7', '8'}\n",
    "    \n",
    "    # 使用 apply 来判断每一行的 'label_sequence' 是否包含所有需要的数字\n",
    "    def contains_all_required_digits(label_sequence):\n",
    "        return all(digit in label_sequence for digit in required_digits)\n",
    "    \n",
    "    # 过滤掉不包含所有需要的数字的行\n",
    "    filtered_df = df[df['label_sequence'].apply(contains_all_required_digits)]\n",
    "    \n",
    "    # 直接保存回原文件\n",
    "    filtered_df.to_csv(file_path, index=False)\n",
    "\n",
    "# 列出所有需要处理的文件路径\n",
    "file_paths = [\n",
    "    'test_data_500.csv',\n",
    "    'train_data_300.csv',\n",
    "    'train_data_500.csv',\n",
    "    'val_data_300.csv',\n",
    "    'val_data_500.csv',\n",
    "    'test_data_300.csv'\n",
    "]\n",
    "\n",
    "# 对每个文件进行处理\n",
    "for file_path in file_paths:\n",
    "    process_file(file_path)\n",
    "\n",
    "print(\"所有文件已更新。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab7406-300d-458a-9b96-16edd7121f49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
